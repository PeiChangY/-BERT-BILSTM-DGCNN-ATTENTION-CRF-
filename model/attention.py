import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class Attention(nn.Module):
    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=12, score_function='bi_linear', dropout=0.5):  #score_function æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°
        ''' Attention Mechanism
        :param embed_dim:
        :param hidden_dim:
        :param out_dim:
        :param n_head: num of head (Multi-Head Attention)
        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)
        :return (?, q_len, out_dim,)
        '''
        super(Attention, self).__init__()
        if hidden_dim is None:
            hidden_dim = embed_dim // n_head
        if out_dim is None:
            out_dim = embed_dim
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.n_head = n_head
        self.score_function = score_function
        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)  #ï¼ˆ22,128ï¼‰
        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)  #ï¼ˆ22,128ï¼‰
        self.proj = nn.Linear(n_head * hidden_dim, out_dim)   #ï¼ˆ128,22ï¼‰
        self.dropout = nn.Dropout(dropout)
        if score_function == 'mlp':
            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))   #è½¬æ¢æˆå¯ä»¥æ”¹å˜å€¼çš„å¼ é‡
        elif self.score_function == 'bi_linear':
            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))
        else:  # dot_product / scaled_dot_product
            self.register_parameter('weight', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.hidden_dim)
        if self.weight is not None:
            self.weight.data.uniform_(-stdv, stdv)

    def forward(self, k, q):
        if len(q.shape) == 2:  # q_len missing   #qä¸ºäºŒç»´æ•°ç»„
            q = torch.unsqueeze(q, dim=1)   #ç»´åº¦æ‰©å¼   åœ¨qä¸­çš„åˆ—ä¸ŠåŠ äº†ä¸€ä¸ªç»´æ•°ä¸º1çš„ç»´åº¦  ä¸‰ç»´
        if len(k.shape) == 2:  # k_len missing
            k = torch.unsqueeze(k, dim=1)
        mb_size = k.shape[0]  # ?       q,kä¸º 16  128  22  k.shape[0]=16
        k_len = k.shape[1]  #128
        q_len = q.shape[1]   #128
        # k: (?, k_len, embed_dim,)   16  128  22
        # q: (?, q_len, embed_dim,)   16  128  22
        # kx: (n_head*?, k_len, hidden_dim)    16  128  128
        # qx: (n_head*?, q_len, hidden_dim)    16  128  128
        # score: (n_head*?, q_len, k_len,)     16  128  128
        # output: (?, q_len, out_dim,)         16  128  22
        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)  #  ï¼ˆ16ï¼Œ128ï¼Œ1ï¼Œ128ï¼‰   viewï¼ˆï¼‰çš„ä½œç”¨æ˜¯æ”¹å˜ä¸ºå¯¹åº”ç»´åº¦
        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)    #permuteå‡½æ•°çš„ä½œç”¨æ˜¯ç»´åº¦æ¢ä½  (1,16,128,128)   contiguous()ä½œç”¨æ˜¯æ‹·è´å¼ é‡    ï¼ˆ16,128,128ï¼‰
        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)
        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)
        if self.score_function == 'dot_product': #åŠ æ€§æ¨¡å‹å’Œç‚¹ç§¯æ¨¡å‹çš„å¤æ‚åº¦ç›¸ä¼¼ï¼Œä½†ç‚¹ç§¯æ¨¡å‹å¯ä»¥æ›´å¥½åˆ©ç”¨çŸ©é˜µè®¡ç®—ï¼Œå…¶æ•ˆç‡æ›´é«˜ï¼›
            kt = kx.permute(0, 2, 1)    #16  128   128
            score = torch.bmm(qx, kt)  # 16  128  128
        elif self.score_function == 'scaled_dot_product':  #ç¼©æ”¾ç‚¹ç§¯æ¨¡å‹æ˜¯ç‚¹ç§¯æ¨¡å‹åœ¨é«˜ç»´è¾“å…¥å‘é‡ä¸Šçš„æ”¹è¿›ï¼Œå®ƒå¯ä»¥è§£å†³å½“è¾“å…¥å‘é‡çš„ç»´åº¦ğ· è¾ƒé«˜æ—¶ï¼Œç‚¹ç§¯æ¨¡å‹è®¡ç®—å€¼æ–¹å·®è¾ƒå¤§ï¼Œä»è€Œå¯¼è‡´Softmaxå‡½æ•°æ¢¯åº¦è¾ƒå°çš„é—®é¢˜ï¼›
            kt = kx.permute(0, 2, 1)  #  16  128  128
            qkt = torch.bmm(qx, kt)   #  16  128  128
            score = torch.div(qkt, math.sqrt(self.hidden_dim)) #math.sqrt(x) å¾—åˆ°xçš„å¹³æ–¹æ ¹   torch.divï¼ˆa,bï¼‰å¾—åˆ°aé™¤ä»¥bçš„ç»“æœ
        elif self.score_function == 'mlp': #åŠ æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŠŠQå’ŒKç»“åˆèµ·æ¥è¾“å…¥ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºä¸­   Qå’ŒKé•¿åº¦ä¸åŒæ—¶æ•ˆæœè¾ƒå¥½
            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1) #expand()å‡½æ•°çš„ä½œç”¨æ˜¯æŠŠæŒ‡å®šç»´åº¦æ‰©å¤§ä¸ºæ›´é«˜ç»´  16  128 128  128
            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)    # 16  128  128  128
            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)  16  128  128  256
            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)
            # score = F.tanh(torch.matmul(kq, self.weight))
            score = torch.tanh(torch.matmul(kq,self.weight))  #matmulï¼ˆï¼‰ä½œç”¨æ˜¯ç»™çŸ©é˜µåšä¹˜æ³•     16  128  128
        elif self.score_function == 'bi_linear':  #åŒçº¿æ€§æ¨¡å‹æ˜¯ä¸€ç§æ³›åŒ–çš„ç‚¹ç§¯æ¨¡å‹ï¼ˆå³ï¼šåˆ†åˆ«å¯¹ğ’™ å’Œğ’’ è¿›è¡Œçº¿æ€§å˜æ¢åè®¡ç®—ç‚¹ç§¯ï¼‰ï¼Œç›¸æ¯”ç‚¹ç§¯æ¨¡å‹ï¼ŒåŒçº¿æ€§æ¨¡å‹åœ¨è®¡ç®—ç‰¹å¾æƒé‡æ—¶å¼•å…¥äº†éå¯¹ç§°æ€§ã€‚
            qw = torch.matmul(qx, self.weight)
            kt = kx.permute(0, 2, 1)
            score = torch.bmm(qw, kt)  #bmmè®¡ç®—ä¸¤ä¸ªtensorçš„çŸ©é˜µä¹˜æ³•ï¼Œtorch.bmm(a,b),tensor a çš„sizeä¸º(b,h,w),tensor bçš„sizeä¸º(b,w,m)  è¾“å‡ºç»´åº¦ ï¼ˆb,h,mï¼‰
        else:
            raise RuntimeError('invalid score_function')
        score = F.softmax(score, dim=-1)    # 16  128  128
        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)  16  128  128
        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)   16  128  128
        output = self.proj(output)  # (?, q_len, out_dim)  16  128  22
        output = self.dropout(output)
        return output, score


class NoQueryAttention(Attention):
    '''q is a parameter'''
    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', q_len=1, dropout=0):
        super(NoQueryAttention, self).__init__(embed_dim, hidden_dim, out_dim, n_head, score_function, dropout)
        self.q_len = q_len
        self.q = nn.Parameter(torch.Tensor(q_len, embed_dim))
        self.reset_q()

    def reset_q(self):
        stdv = 1. / math.sqrt(self.embed_dim)
        self.q.data.uniform_(-stdv, stdv)

    def forward(self, k, **kwargs):
        mb_size = k.shape[0]
        q = self.q.expand(mb_size, -1, -1)
        return super(NoQueryAttention, self).forward(k, q)
